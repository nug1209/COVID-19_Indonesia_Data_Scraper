{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tabula import read_pdf\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "# pdf examples 090520 080920 040720 120420 170620 210320 240820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ PDF\n",
    "\n",
    "def get_table(file_number):\n",
    "\n",
    "    table = read_pdf(directory + '/DL' + file_number + '.pdf', pages='2')\n",
    "    #table[0]\n",
    "    df = pd.DataFrame(table[0])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET INDEX OF 'ACEH'\n",
    "\n",
    "def getIndexes(dfObj, value):\n",
    "    listOfPos = list()\n",
    "    result = dfObj.isin([value])\n",
    "    seriesObj = result.any()\n",
    "    columnNames = list(seriesObj[seriesObj == True].index)\n",
    "    for col in columnNames:\n",
    "        rows = list(result[col][result[col] == True].index)\n",
    "        for row in rows:\n",
    "            listOfPos.append((row, col))\n",
    "    return listOfPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(df):\n",
    "\n",
    "    pos_aceh = getIndexes(df,'ACEH')[0][0]\n",
    "\n",
    "    # DROP SOME UNUSED ROWS AND COLUMNS\n",
    "\n",
    "    drop_columns1 = [df.columns[0], df.columns[2]]\n",
    "    drop_rows = list(range(0, pos_aceh))\n",
    "    df = df.drop(drop_rows)\n",
    "    df = df.drop(labels=drop_columns1, axis=1)\n",
    "\n",
    "    # RENAME COLUMNS 'PROVINSI' AND COLUMS THAT CONTAIN THE NUMBERS\n",
    "\n",
    "    for i in range(len(df.columns)):\n",
    "        if i == 0 : df.rename(columns={df.columns[i]: 'PROVINSI'}, inplace=True)\n",
    "        else :\n",
    "                df.rename(columns={df.columns[i]: 'TEMP' + str(i)}, inplace=True)\n",
    "\n",
    "    # SPLIT THE NUMBERS IN THE CONTAINING COLUMNS AND PLACE THE NUMBERS ON SEPARATE COLUMNS\n",
    "\n",
    "    for i in np.arange(1, len(df.columns)):\n",
    "        s = df['TEMP' + str(i)].str.split(' ', expand = True)\n",
    "        y = 0\n",
    "        for z in np.arange(len(s.columns)):\n",
    "            df[str(i) + '-' + str(z)] = s[y]\n",
    "            y = y + 1\n",
    "\n",
    "    # DROP COLUMNS CONTAINING ONLY RESULTING FROM THE SPLITTING PROCESS\n",
    "\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "    df.dropna(how='all', axis=1, inplace=True)\n",
    "\n",
    "    # DROP TEMP COLUMNS\n",
    "\n",
    "    columns_list = list(df.columns)\n",
    "    columns_list\n",
    "    temp_index = [i for i, word in enumerate(columns_list) if word.startswith('TEMP')]\n",
    "    temp_drops = []\n",
    "    \n",
    "    for i in temp_index:\n",
    "        temp_drops.append(df.columns[i])\n",
    "\n",
    "    df = df.drop(temp_drops, axis=1)\n",
    "\n",
    "    # RENAME 'TOTAL' ROW AND ADD 'TGL' COLUMN\n",
    "\n",
    "    df.iloc[-1][0] = 'TOTAL'\n",
    "    df[\"TGL\"] = file_number[:2] + \"-\" + file_number[2:4] + \"-\" + file_number[4:]\n",
    "\n",
    "    [parser.parse(i) for i in df['TGL']]\n",
    "    df['TGL'] = [datetime.strptime(i, '%d-%m-%y') for i in df['TGL']]\n",
    "    df['TGL'] = [i.date() for i in df['TGL']]\n",
    "\n",
    "    # RENAME NUMBER COLUMNS NAMES\n",
    "\n",
    "    number_column_names = ['PSTF H-1', 'PSTF', 'PSTF KUM', 'SMBH H-1', 'SMBH', 'SMBH KUM',\n",
    "                        'MNGL H-1', 'MNGL', 'MNGL KUM']\n",
    "\n",
    "    for i in np.arange(1, len(df.columns)-1):\n",
    "        df = df.rename(columns={df.columns[i]:number_column_names[i-1]})\n",
    "\n",
    "    # CLEAN ROWS AT THE END\n",
    "\n",
    "    pos_gorontalo = getIndexes(df,'GORONTALO')[0][0]\n",
    "    pos_total = getIndexes(df, 'TOTAL')[0][0]\n",
    "    pos_total\n",
    "\n",
    "    drop_rows_end = np.arange(pos_gorontalo + 1, pos_total)\n",
    "    df = df.drop(drop_rows_end)\n",
    "\n",
    "    # REMOVE MULTIPLE WHITESPACES ON 'PROVINSI'\n",
    "    \n",
    "    df = df.replace(to_replace=r'\\s\\s+', value=' ', regex=True)\n",
    "    \n",
    "    # ADD 'DPRIKS' COLUMN\n",
    "\n",
    "    df['DPRIKS'] = np.NaN\n",
    "\n",
    "    # GET THE 'DIPERIKSA' DATA\n",
    "    # READ PDF SPECIFIC AREA OF PAGE 1\n",
    "\n",
    "    box = [212.00, 325.00, 300.00, 600.00]\n",
    "    extract_data = read_pdf(directory + '/DL' + file_number + '.pdf', pages='1', area=[box], stream=True)\n",
    "\n",
    "    # FIND THE 'DIPERIKSA' DATA AND CLEAN IT\n",
    "\n",
    "    string_extract = ''.join(map(str, extract_data))\n",
    "    diperiksa_find = '(diperiksa?\\s*:\\s*[0-9.,]+)'\n",
    "    diperiksa_data = re.findall(diperiksa_find, string_extract)\n",
    "    diperiksa_data = diperiksa_data[0].replace(',', '').replace('.','').replace('diperiksa','').replace(':','').strip()\n",
    "    diperiksa_data = int(diperiksa_data)\n",
    "\n",
    "    # ADD 'DIPERIKSA' DATA TO TABLE\n",
    "\n",
    "    df.at[df[df['PROVINSI']=='TOTAL'].index.values.astype(int), 'DPRIKS'] = diperiksa_data\n",
    "\n",
    "    # RESET INDEX\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # CONVERT ALL COLUMN TYPES\n",
    "\n",
    "    convert_columns = {'PSTF H-1': int, 'PSTF': int, 'PSTF KUM': int, 'SMBH H-1': int, 'SMBH': int,\n",
    "                       'SMBH KUM': int, 'MNGL H-1': int, 'MNGL': int, 'MNGL KUM': int} \n",
    "    df = df.astype(convert_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_excel(df):\n",
    "    \n",
    "    # SAVE RAW DATA TABLE TO EXCEL FILE\n",
    " \n",
    "    df.to_excel(directory + '/RAW' + file_number + '.xlsx')\n",
    "    print(\"processing RAW\" + file_number + \" done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_table(df):\n",
    "    \n",
    "    # DROP MORE UNUSED ROWS AND COLUMNS\n",
    "\n",
    "    drop_columns2 = [df.columns[1], df.columns[4], df.columns[7], df.columns[11]]\n",
    "    df_clean = df.drop(labels=drop_columns2, axis=1)\n",
    "    df_clean = df_clean.drop(getIndexes(df,'TOTAL')[0][0])\n",
    "    \n",
    "#     for count, entry in enumerate(df['PROVINSI']):\n",
    "#         sub = re.sub('\\s\\s+', ' ', entry)\n",
    "#         df['PROVINSI'][count] = sub\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_to_excel(df_clean):\n",
    "    \n",
    "#  SAVE CLEAN DATA TABLE TO EXCEL FILE\n",
    "\n",
    "    df_clean.to_excel(directory + '/CL' + file_number + '.xlsx')\n",
    "    print('processing CL' + file_number + ' done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CFR = MNGL/PSTF x 100% ... case fatality rate\n",
    "# PR = PSTF/DPRIKS x 100% ... positivity rate\n",
    "# NGTF = DPRIKS - PSTF\n",
    "\n",
    "def get_total(df):\n",
    "    \n",
    "    # CREATE DATAFRAME FOR TOTAL NATIONAL NUMBERS\n",
    "\n",
    "    df_total = df[(df['PROVINSI'] == 'TOTAL')]\n",
    "    drop_columns3 = [df.columns[0], df.columns[1], df.columns[4], df.columns[7]]\n",
    "    df_total = df_total.drop(labels=drop_columns3, axis=1)\n",
    "    df_total['DPRIKS'] = df_total['DPRIKS'].astype(int)\n",
    "\n",
    "    # CALCULATE INDICES\n",
    "\n",
    "    df_total['PR KUM'] = 100 * df_total['PSTF KUM'] / df_total['DPRIKS']\n",
    "    df_total['CFR HRN'] = 100 * df_total['MNGL'] / df_total['PSTF']\n",
    "    df_total['CFR KUM'] = 100 * df_total['MNGL KUM'] / df_total['PSTF KUM']\n",
    "#     df_total['NGTF'] = df_total['DPRIKS'] - df_total['PSTF']\n",
    "\n",
    "    #  REORDER COLUMNS\n",
    "\n",
    "    df_total = df_total[['DPRIKS', 'PSTF', 'PSTF KUM', 'SMBH', 'SMBH KUM', 'MNGL', 'MNGL KUM', 'PR KUM',\n",
    "                         'CFR HRN', 'CFR KUM', 'TGL']]\n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all(files):\n",
    "    df = get_table(file_number)\n",
    "    df = process_table(df)\n",
    "    df_clean = clean_table(df)\n",
    "    df_total = get_total(df)\n",
    "    raw_to_excel(df)\n",
    "    clean_to_excel(df_clean)\n",
    "    return df_total, df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 091020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Oct 14, 2020 4:32:21 PM org.apache.pdfbox.pdfparser.COSParser parseXref\n",
      "WARNING: /XRefStm offset 616802 is incorrect, corrected to 616808\n",
      "Oct 14, 2020 4:32:22 PM org.apache.pdfbox.pdfparser.COSParser validateStreamLength\n",
      "WARNING: The end of the stream doesn't point to the correct offset, using workaround to read the stream, stream start position: 613671, length: 3064, expected end position: 616735\n",
      "\n",
      "Got stderr: Oct 14, 2020 4:32:24 PM org.apache.pdfbox.pdfparser.COSParser parseXref\n",
      "WARNING: /XRefStm offset 616802 is incorrect, corrected to 616808\n",
      "Oct 14, 2020 4:32:25 PM org.apache.pdfbox.pdfparser.COSParser validateStreamLength\n",
      "WARNING: The end of the stream doesn't point to the correct offset, using workaround to read the stream, stream start position: 613671, length: 3064, expected end position: 616735\n",
      "Oct 14, 2020 4:32:25 PM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
      "WARNING: Format 14 cmap table is not supported and will be ignored\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing RAW091020 done!\n",
      "processing CL091020 done!\n",
      "Processing 101020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Oct 14, 2020 4:32:25 PM org.apache.pdfbox.pdfparser.COSParser parseXref\n",
      "WARNING: /XRefStm offset 616947 is incorrect, corrected to 616953\n",
      "Oct 14, 2020 4:32:26 PM org.apache.pdfbox.pdfparser.COSParser validateStreamLength\n",
      "WARNING: The end of the stream doesn't point to the correct offset, using workaround to read the stream, stream start position: 613816, length: 3064, expected end position: 616880\n",
      "\n",
      "Got stderr: Oct 14, 2020 4:32:28 PM org.apache.pdfbox.pdfparser.COSParser parseXref\n",
      "WARNING: /XRefStm offset 616947 is incorrect, corrected to 616953\n",
      "Oct 14, 2020 4:32:28 PM org.apache.pdfbox.pdfparser.COSParser validateStreamLength\n",
      "WARNING: The end of the stream doesn't point to the correct offset, using workaround to read the stream, stream start position: 613816, length: 3064, expected end position: 616880\n",
      "Oct 14, 2020 4:32:28 PM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
      "WARNING: Format 14 cmap table is not supported and will be ignored\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing RAW101020 done!\n",
      "processing CL101020 done!\n",
      "Processing 111020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: Oct 14, 2020 4:32:29 PM org.apache.pdfbox.pdfparser.COSParser parseXref\n",
      "WARNING: /XRefStm offset 617437 is incorrect, corrected to 617443\n",
      "Oct 14, 2020 4:32:29 PM org.apache.pdfbox.pdfparser.COSParser validateStreamLength\n",
      "WARNING: The end of the stream doesn't point to the correct offset, using workaround to read the stream, stream start position: 614306, length: 3064, expected end position: 617370\n",
      "\n",
      "Got stderr: Oct 14, 2020 4:32:32 PM org.apache.pdfbox.pdfparser.COSParser parseXref\n",
      "WARNING: /XRefStm offset 617437 is incorrect, corrected to 617443\n",
      "Oct 14, 2020 4:32:32 PM org.apache.pdfbox.pdfparser.COSParser validateStreamLength\n",
      "WARNING: The end of the stream doesn't point to the correct offset, using workaround to read the stream, stream start position: 614306, length: 3064, expected end position: 617370\n",
      "Oct 14, 2020 4:32:32 PM org.apache.fontbox.ttf.CmapSubtable processSubtype14\n",
      "WARNING: Format 14 cmap table is not supported and will be ignored\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing RAW111020 done!\n",
      "processing CL111020 done!\n",
      "Processing 121020\n",
      "list index out of range\n"
     ]
    }
   ],
   "source": [
    "error_files = []\n",
    "error_msgs = []\n",
    "\n",
    "# SET THE FOLDER NAME\n",
    "\n",
    "month_name = 'Oktober'\n",
    "directory = 'Downloaded/' + month_name\n",
    "\n",
    "# PREPARE ARRAYS WHERE THE DATA IS POPULATED\n",
    "\n",
    "clean_columns = ['PROVINSI', 'PSTF', 'PSTF KUM', 'SMBH', 'SMBH KUM', 'MNGL', 'MNGL KUM', 'TGL']\n",
    "total_columns = ['DPRIKS', 'PSTF', 'PSTF KUM', 'SMBH', 'SMBH KUM', 'MNGL', 'MNGL KUM', 'PR KUM', 'CFR HRN', 'CFR KUM', 'TGL']\n",
    "\n",
    "agg_df_clean = pd.DataFrame(columns=clean_columns)\n",
    "agg_df_total = pd.DataFrame(columns=total_columns)\n",
    "\n",
    "# IMPLEMENT FUNCTION WITH ITERATION OF FILES IN FOLDER\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    file_number = file[2:8]\n",
    "    print('Processing ' + file_number)\n",
    "    try:\n",
    "        df_total, df_clean = process_all(file_number)\n",
    "        agg_df_clean = agg_df_clean.append(df_clean, ignore_index=True)\n",
    "        agg_df_total = agg_df_total.append(df_total, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        error_files.append(file_number)\n",
    "        error_msgs.append(e)\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "# HANDLE LOG OF ERRORS\n",
    "\n",
    "error_dict = {'Error Files': error_files, 'Error Messages': error_msgs}\n",
    "df_error = pd.DataFrame(data=error_dict)\n",
    "df_error.to_excel(directory + '/ErrorLog_' + month_name + '.xlsx')\n",
    "\n",
    "# WRITE AGGREGATED DATA TO EXCEL\n",
    "\n",
    "agg_df_clean.to_excel(directory + '/AggCL_' + month_name + '.xlsx')\n",
    "agg_df_total.to_excel(directory + '/AggTTL_' + month_name + '.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
