{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tabula import read_pdf\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "# pdf examples 090520 080920 040720 120420 170620 210320 240820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ CSV\n",
    "\n",
    "def get_table(file_number):\n",
    "\n",
    "    df = pd.read_csv(directory + '/tabula-DL' + file_number + '.csv')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET INDEX OF 'ACEH'\n",
    "\n",
    "def getIndexes(dfObj, value):\n",
    "    listOfPos = list()\n",
    "    result = dfObj.isin([value])\n",
    "    seriesObj = result.any()\n",
    "    columnNames = list(seriesObj[seriesObj == True].index)\n",
    "    for col in columnNames:\n",
    "        rows = list(result[col][result[col] == True].index)\n",
    "        for row in rows:\n",
    "            listOfPos.append((row, col))\n",
    "    return listOfPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_table(df):\n",
    "\n",
    "    pos_aceh = getIndexes(df,'ACEH')[0][0]\n",
    "\n",
    "    # DROP SOME UNUSED ROWS AND COLUMNS\n",
    "\n",
    "    drop_columns1 = [df.columns[0], df.columns[2]]\n",
    "    drop_rows = list(range(0, pos_aceh))\n",
    "    df = df.drop(drop_rows)\n",
    "    df = df.drop(labels=drop_columns1, axis=1)\n",
    "\n",
    "    # RENAME COLUMNS 'PROVINSI' AND COLUMS THAT CONTAIN THE NUMBERS\n",
    "\n",
    "    for i in range(len(df.columns)):\n",
    "        if i == 0 : df.rename(columns={df.columns[i]: 'PROVINSI'}, inplace=True)\n",
    "        else :\n",
    "                df.rename(columns={df.columns[i]: 'TEMP' + str(i)}, inplace=True)\n",
    "\n",
    "    # SPLIT THE NUMBERS IN THE CONTAINING COLUMNS AND PLACE THE NUMBERS ON SEPARATE COLUMNS\n",
    "\n",
    "    for i in np.arange(1, len(df.columns)):\n",
    "        s = df['TEMP' + str(i)].str.split(' ', expand = True)\n",
    "        y = 0\n",
    "        for z in np.arange(len(s.columns)):\n",
    "            df[str(i) + '-' + str(z)] = s[y]\n",
    "            y = y + 1\n",
    "\n",
    "    # DROP COLUMNS CONTAINING ONLY RESULTING FROM THE SPLITTING PROCESS\n",
    "\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "    df.dropna(how='all', axis=1, inplace=True)\n",
    "\n",
    "    # DROP TEMP COLUMNS\n",
    "\n",
    "    columns_list = list(df.columns)\n",
    "    columns_list\n",
    "    temp_index = [i for i, word in enumerate(columns_list) if word.startswith('TEMP')]\n",
    "    temp_drops = []\n",
    "    \n",
    "    for i in temp_index:\n",
    "        temp_drops.append(df.columns[i])\n",
    "\n",
    "    df = df.drop(temp_drops, axis=1)\n",
    "\n",
    "    # RENAME 'TOTAL' ROW AND ADD 'TGL' COLUMN\n",
    "\n",
    "    df.iloc[-1][0] = 'TOTAL'\n",
    "    df[\"TGL\"] = file_number[:2] + \"-\" + file_number[2:4] + \"-\" + file_number[4:]\n",
    "\n",
    "    [parser.parse(i) for i in df['TGL']]\n",
    "    df['TGL'] = [datetime.strptime(i, '%d-%m-%y') for i in df['TGL']]\n",
    "    df['TGL'] = [i.date() for i in df['TGL']]\n",
    "\n",
    "    # RENAME NUMBER COLUMNS NAMES\n",
    "\n",
    "    number_column_names = ['PSTF H-1', 'PSTF', 'PSTF KUM', 'SMBH H-1', 'SMBH', 'SMBH KUM',\n",
    "                        'MNGL H-1', 'MNGL', 'MNGL KUM']\n",
    "\n",
    "    for i in np.arange(1, len(df.columns)-1):\n",
    "        df = df.rename(columns={df.columns[i]:number_column_names[i-1]})\n",
    "\n",
    "    # CLEAN ROWS AT THE END\n",
    "\n",
    "    pos_gorontalo = getIndexes(df,'GORONTALO')[0][0]\n",
    "    pos_total = getIndexes(df, 'TOTAL')[0][0]\n",
    "    pos_total\n",
    "\n",
    "    drop_rows_end = np.arange(pos_gorontalo + 1, pos_total)\n",
    "    df = df.drop(drop_rows_end)\n",
    "\n",
    "    # REMOVE MULTIPLE WHITESPACES ON 'PROVINSI'\n",
    "    \n",
    "    df = df.replace(to_replace=r'\\s\\s+', value=' ', regex=True)\n",
    "    \n",
    "    # RESET INDEX\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # CONVERT ALL COLUMN TYPES\n",
    "\n",
    "    convert_columns = {'PSTF H-1': int, 'PSTF': int, 'PSTF KUM': int, 'SMBH H-1': int, 'SMBH': int,\n",
    "                       'SMBH KUM': int, 'MNGL H-1': int, 'MNGL': int, 'MNGL KUM': int} \n",
    "    df = df.astype(convert_columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_table(df):\n",
    "    \n",
    "    # DROP MORE UNUSED ROWS AND COLUMNS\n",
    "\n",
    "    drop_columns2 = [df.columns[1], df.columns[4], df.columns[7]]\n",
    "    df_clean = df.drop(labels=drop_columns2, axis=1)\n",
    "    df_clean = df_clean.drop(getIndexes(df,'TOTAL')[0][0])\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_to_excel(df_clean):\n",
    "    \n",
    "#  SAVE CLEAN DATA TABLE TO EXCEL FILE\n",
    "\n",
    "    df_clean.to_excel(directory + '/CL' + file_number + '.xlsx')\n",
    "    print('processing CL' + file_number + ' done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all(files):\n",
    "    df = get_table(file_number)\n",
    "    df = process_table(df)\n",
    "    df_clean = clean_table(df)\n",
    "    clean_to_excel(df_clean)\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 110520\n",
      "processing CL110520 done!\n",
      "Processing 270520\n",
      "processing CL270520 done!\n"
     ]
    }
   ],
   "source": [
    "error_files = []\n",
    "error_msgs = []\n",
    "\n",
    "# SET THE FOLDER NAME\n",
    "\n",
    "month_name = 'Mei'\n",
    "directory = 'TabulaCSV/' + month_name\n",
    "\n",
    "# PREPARE ARRAYS WHERE THE DATA IS POPULATED\n",
    "\n",
    "clean_columns = ['PROVINSI', 'PSTF', 'PSTF KUM', 'SMBH', 'SMBH KUM', 'MNGL', 'MNGL KUM', 'TGL']\n",
    "\n",
    "agg_df_clean = pd.DataFrame(columns=clean_columns)\n",
    "\n",
    "# IMPLEMENT FUNCTION WITH ITERATION OF FILES IN FOLDER\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    file_number = file[9:15]\n",
    "    print('Processing ' + file_number)\n",
    "    try:\n",
    "        df_clean = process_all(file_number)\n",
    "        agg_df_clean = agg_df_clean.append(df_clean, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        error_files.append(file_number)\n",
    "        error_msgs.append(e)\n",
    "        print(e)\n",
    "        continue\n",
    "        \n",
    "# HANDLE LOG OF ERRORS\n",
    "\n",
    "error_dict = {'Error Files': error_files, 'Error Messages': error_msgs}\n",
    "df_error = pd.DataFrame(data=error_dict)\n",
    "df_error.to_excel(directory + '/ErrorLog_' + month_name + '.xlsx')\n",
    "\n",
    "# WRITE AGGREGATED DATA TO EXCEL\n",
    "\n",
    "agg_df_clean.to_excel(directory + '/AggCL_' + month_name + '.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
